numGameThreads = 128

# GPU Settings-------------------------------------------------------------------------------
# 1xA6000
# 2 sockets of AMD EPYC 7763 64-Core Processors = 128 physical cores
#                                                 each core has 2 threads

nnMaxBatchSize = 128
nnCacheSizePowerOfTwo = 21
nnMutexPoolSizePowerOfTwo = 17
numNNServerThreadsPerModel = 1
nnRandomize = true

# CUDA GPU settings--------------------------------------
# cudaDeviceToUse = 0 #use device 0 for all server threads (numNNServerThreadsPerModel) unless otherwise specified per-model or per-thread-per-model
# cudaDeviceToUseModel0 = 3 #use device 3 for model 0 for all threads unless otherwise specified per-thread for this model
# cudaDeviceToUseModel1 = 2 #use device 2 for model 1 for all threads unless otherwise specified per-thread for this model
# cudaDeviceToUseModel0Thread0 = 3 #use device 3 for model 0, server thread 0
# cudaDeviceToUseModel0Thread1 = 2 #use device 2 for model 0, server thread 1

deviceToUseThread0 = 0

useFP16 = true
# CUDA runs ~30% faster with NHWC on A6000 GPUs. The TorchScript backend
# requires NCHW, however, so if nnModelFileN is a TorchScript file, then
# useNHWCN and inputsUseNHWCN should be set to false.
useNHWC = false
inputsUseNHWC = false
