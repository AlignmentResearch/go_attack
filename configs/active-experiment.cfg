# Must be combined with a config file from the compute/ folder to work
@include amcts/victimplay.cfg

maxVisits0 = 32 # increase this to make victim stronger
# Uncomment this for pass-hardening
# passingBehavior0 = avoid-pass-alive-territory

# Normally optimism is 0 during training, but we'll set the victim's optimism to the
# levels seen in eval.
policyOptimism0=1.0
rootPolicyOptimism0=0.2

# Increase the proportion of 19x19 games.
bSizeRelProbs = 1,1,4,2,3,4,10,6,7,8,9,10,75

# LCB is turned off for selfplay since lightvector found that LCB make training
# progress slightly worse, though LCB makes eval much stronger:
# https://lifein19x19.com/viewtopic.php?p=278323#p278323
# For victimplay it's a bit disconcerting to see a large train/eval gap, and we
# probably compensate for worse training progress by moving along the curriculum
# slower and hence using fewer victim visits for longer.
useLcbForSelfplayMove = true

# Set victim config parameters to eval settings to reduce train-eval gap.
# (Adversary parameters also contribute to the train-eval gap. I'm more
# hesitant to change adversary parameters, however, since they're more likely to
# be important for stable and efficient training.)
chosenMoveTemperature0 = 0.10
chosenMoveTemperatureEarly0 = 0.50
# Not setting this param because otherwise I get this error:
#   void extractPolicyTarget(std::vector<PolicyTargetMove>&, const Search*, const SearchNode*, std::vector<short int>&, std::vector<double>&): Assertion `!toMoveBot->searchParams.rootSymmetryPruning' failed.
# Maybe we can avoid this function call for the victim because I wouldn't think
# the policy target matters for the victim, but I'm not interested in spending
# time investigating that change unless it's really important.
# rootSymmetryPruning0 = true
antiMirror0 = true
fillDameBeforePass0 = true
conservativePass0 = true
rootNoiseEnabled0 = false
staticScoreUtilityFactor0 = 0.1
dynamicScoreUtilityFactor0 = 0.3
dynamicScoreCenterZeroWeight0 = 0.2
dynamicScoreCenterScale0 = 0.75
rootPolicyTemperatureEarly0 = 1.0
rootPolicyTemperature0 = 1.0
cpuctExploration0 = 1.0
cpuctExplorationLog0 = 0.45
cpuctUtilityStdevScale0 = 0.85
useNoisePruning0 = true
useUncertainty0 = true
useNonBuggyLcb0 = true
# additional settings that I missed before
valueWeightExponent0 = 0.25
rootFpuReductionMax0 = 0.1
rootNumSymmetriesToSample0 = 1
rootDesiredPerChildVisitsCoeff0 = 0
enablePassingHacks0 = true
subtreeValueBiasFactor0 = 0.45
subtreeValueBiasWeightExponent0 = 0.85

# Adjusting some adversary params to be closer to parameters used in latest
# KataGo training runs.
staticScoreUtilityFactor1 = 0.05
dynamicScoreUtilityFactor1 = 0.30
dynamicScoreCenterScale1 = 0.75
rootPolicyTemperatureEarly1 = 1.5
rootPolicyTemperature1 = 1.1
cpuctExploration1 = 1.05
cpuctExplorationLog1 = 0.28
subtreeValueBiasFactor1 = 0.30
useNonBuggyLcb1 = true

# Let's penalize long games.
# The attack-b18 adversary has learned to play extremely long games (one eval
# game dragged out for 13000 moves), which we don't want:
# - by default, when a game hits the turn limit, the training value targets are
#   populated by scoring the final board state. It's possible that the adversary
#   is learning to stall in positions that are winning if they are immediately
#   scored but are losing if fully played out.
# - the training data becomes flooded with end-game data. Maybe this is bad for
#   training.
maxMovesPerGame = 900
scaleMaxMovesWithBoardSize=true
hitTurnLimitIsNoResult=true
forceAllowNoResultPredictions1=true
# Harshly penalize no-result to make it worse than losing every point on the
# board (-1.35 utility).
noResultUtility1=-1.6
